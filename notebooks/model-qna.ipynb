{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-10T15:23:51.474862Z",
     "iopub.status.busy": "2025-05-10T15:23:51.474589Z",
     "iopub.status.idle": "2025-05-10T15:23:51.733270Z",
     "shell.execute_reply": "2025-05-10T15:23:51.732703Z",
     "shell.execute_reply.started": "2025-05-10T15:23:51.474839Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/introducing-quail-a-comprehensive-reading-compre/validation.csv\n",
      "/kaggle/input/introducing-quail-a-comprehensive-reading-compre/train.csv\n",
      "/kaggle/input/introducing-quail-a-comprehensive-reading-compre/challenge.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T15:23:51.734502Z",
     "iopub.status.busy": "2025-05-10T15:23:51.734186Z",
     "iopub.status.idle": "2025-05-10T15:23:51.737792Z",
     "shell.execute_reply": "2025-05-10T15:23:51.737243Z",
     "shell.execute_reply.started": "2025-05-10T15:23:51.734484Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-05-10T15:23:51.751141Z",
     "iopub.status.busy": "2025-05-10T15:23:51.750915Z",
     "iopub.status.idle": "2025-05-10T15:25:20.343479Z",
     "shell.execute_reply": "2025-05-10T15:25:20.342391Z",
     "shell.execute_reply.started": "2025-05-10T15:23:51.751125Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.5.1+cu124)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\n",
      "Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.9.90\n",
      "    Uninstalling nvidia-curand-cu12-10.3.9.90:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed bitsandbytes-0.45.5 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=6cf32ccf91c17afd86516c98cb0dafa1dcf091e5e6ac6d6387b7fe1b57dbab32\n",
      "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: fsspec, rouge_score, evaluate\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\n",
      "bigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed evaluate-0.4.3 fsspec-2024.12.0 rouge_score-0.1.2\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.30.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U bitsandbytes\n",
    "!pip install transformers datasets torch accelerate evaluate rouge_score\n",
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T15:38:25.706898Z",
     "iopub.status.busy": "2025-05-10T15:38:25.706230Z",
     "iopub.status.idle": "2025-05-10T15:38:25.711318Z",
     "shell.execute_reply": "2025-05-10T15:38:25.710606Z",
     "shell.execute_reply.started": "2025-05-10T15:38:25.706874Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers  import BitsAndBytesConfig\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from huggingface_hub import login\n",
    "import torch.nn as nn\n",
    "import re\n",
    "import pandas as pd\n",
    "import re\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T15:38:26.753308Z",
     "iopub.status.busy": "2025-05-10T15:38:26.753032Z",
     "iopub.status.idle": "2025-05-10T15:38:26.854635Z",
     "shell.execute_reply": "2025-05-10T15:38:26.853920Z",
     "shell.execute_reply.started": "2025-05-10T15:38:26.753291Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Replace with your Hugging Face token\n",
    "login(token=\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def fix_answer_string(answer_str):\n",
    "    if not isinstance(answer_str, str):\n",
    "        return []\n",
    "    cleaned = re.sub(r\"'\\s+'\", \"', '\", answer_str)\n",
    "    try:\n",
    "        return eval(cleaned)\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def get_target_text(row):\n",
    "    try:\n",
    "        if isinstance(row[\"answers\"], list) and 0 <= row[\"correct_answer_id\"] < len(row[\"answers\"]):\n",
    "            return row[\"answers\"][row[\"correct_answer_id\"]]\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def reformat_df_cleaned(df_cleaned):\n",
    "    df_reformatted = df_cleaned.copy()\n",
    "    df_reformatted['parsed_answer'] = df_reformatted['answers'].apply(\n",
    "        lambda x: x if isinstance(x, str) else \"not enough information\"\n",
    "    )\n",
    "    df_reformatted['prompt'] = (\n",
    "        \"### Context:\\n\" + df_reformatted['context'] + \"\\n\\n\" +\n",
    "        \"### Question:\\n\" + df_reformatted['question'] + \"\\n\\n\" +\n",
    "        \"### Instruction:\\nProvide the answer to the question based on the context.\\n\\n\" +\n",
    "        \"### Answer:\\n\" + df_reformatted['parsed_answer']\n",
    "    )\n",
    "    df_reformatted = df_reformatted.drop(columns=['parsed_answer'])\n",
    "    return df_reformatted\n",
    "\n",
    "def tokenize_function(example):\n",
    "    prompt = example[\"prompt\"]\n",
    "    tokenized = tokenizer(prompt, padding=\"max_length\", truncation=True, max_length=256)\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()  # Causal LM learns from full input\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T15:38:26.985287Z",
     "iopub.status.busy": "2025-05-10T15:38:26.985062Z",
     "iopub.status.idle": "2025-05-10T15:38:41.496015Z",
     "shell.execute_reply": "2025-05-10T15:38:41.495324Z",
     "shell.execute_reply.started": "2025-05-10T15:38:26.985269Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f87f00c151b4519995f5645526faebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b2bc2c60014aceb6280770fb08ee1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8afee1e4243b470ebbd93e741a1cb3cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ef8c07c4fc74501af88e93eee508b80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f07511b231d040859d6b892af5f5af0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6260 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fdacf5f68434ae8bbdb76570584899b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1566 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(\"/kaggle/input/introducing-quail-a-comprehensive-reading-compre/train.csv\")\n",
    "df_cleaned = df[[\"context\", \"question\", \"answers\", \"correct_answer_id\"]]\n",
    "df_cleaned = df_cleaned[df_cleaned['question'].str.endswith('?', na=False)]\n",
    "df_cleaned = df_cleaned.reset_index(drop=True)\n",
    "\n",
    "df_cleaned[\"answers\"] = df_cleaned[\"answers\"].apply(fix_answer_string)\n",
    "df_cleaned[\"answers\"] = df_cleaned.apply(get_target_text, axis=1)\n",
    "\n",
    "train_df, val_df = train_test_split(df_cleaned, test_size=0.2, random_state=42)\n",
    "train_df_qa_answer = reformat_df_cleaned(train_df)\n",
    "val_df_qa_answer = reformat_df_cleaned(val_df)\n",
    "\n",
    "train_dataset_qa_answer = Dataset.from_pandas(train_df_qa_answer)\n",
    "val_dataset_qa_answer = Dataset.from_pandas(val_df_qa_answer)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  \n",
    "\n",
    "train_qa_answer_tokenized = train_dataset_qa_answer.map(\n",
    "    tokenize_function,\n",
    "    remove_columns=[\"prompt\", \"context\", \"question\", \"answers\", \"correct_answer_id\"]\n",
    ")\n",
    "\n",
    "val_qa_answer_tokenized = val_dataset_qa_answer.map(\n",
    "    tokenize_function,\n",
    "    remove_columns=[\"prompt\", \"context\", \"question\", \"answers\", \"correct_answer_id\"]\n",
    ")\n",
    "\n",
    "train_qa_answer_tokenized.set_format(\"torch\")\n",
    "val_qa_answer_tokenized.set_format(\"torch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T15:38:42.552875Z",
     "iopub.status.busy": "2025-05-10T15:38:42.552391Z",
     "iopub.status.idle": "2025-05-10T15:38:42.558192Z",
     "shell.execute_reply": "2025-05-10T15:38:42.557495Z",
     "shell.execute_reply.started": "2025-05-10T15:38:42.552851Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T15:38:43.786100Z",
     "iopub.status.busy": "2025-05-10T15:38:43.785837Z",
     "iopub.status.idle": "2025-05-10T15:39:59.732102Z",
     "shell.execute_reply": "2025-05-10T15:39:59.731370Z",
     "shell.execute_reply.started": "2025-05-10T15:38:43.786084Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b93612c51fd24f188bf10729de3f4454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b18dfab5351d48b3a2968350c724c801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db757ab5eec44876834baae8686480f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d367dace70fa435e98786424b27df5e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26075a7643f741d7a8b9bc7890b5a576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06920ff5438e49af8cafeab23ad40071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fd89951e60f44cabd455fde2c3e5474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"cuda:0\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T15:39:59.733503Z",
     "iopub.status.busy": "2025-05-10T15:39:59.733239Z",
     "iopub.status.idle": "2025-05-10T15:39:59.848887Z",
     "shell.execute_reply": "2025-05-10T15:39:59.848161Z",
     "shell.execute_reply.started": "2025-05-10T15:39:59.733475Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T15:40:13.097664Z",
     "iopub.status.busy": "2025-05-10T15:40:13.096906Z",
     "iopub.status.idle": "2025-05-10T15:40:13.105278Z",
     "shell.execute_reply": "2025-05-10T15:40:13.104624Z",
     "shell.execute_reply.started": "2025-05-10T15:40:13.097641Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "batch_size = 4\n",
    "gradient_accumulation_steps = 16\n",
    "num_train_epochs = 3\n",
    "learning_rate = 2e-4\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "output_dir = \"./mistral_finetuned\"\n",
    "\n",
    "train_dataloader_qa_answer = DataLoader(\n",
    "    train_qa_answer_tokenized,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    ")\n",
    "val_dataloader_qa_answer = DataLoader(\n",
    "    val_qa_answer_tokenized,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T15:40:22.438410Z",
     "iopub.status.busy": "2025-05-10T15:40:22.438117Z",
     "iopub.status.idle": "2025-05-10T15:40:22.448271Z",
     "shell.execute_reply": "2025-05-10T15:40:22.447441Z",
     "shell.execute_reply.started": "2025-05-10T15:40:22.438389Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, val_dataloader, output_dir, task_name, repo_name, access_token,\n",
    "                tokenizer, optimizer, num_train_epochs, gradient_accumulation_steps, device):\n",
    "    model.train()\n",
    "    total_steps = (len(train_dataloader) // gradient_accumulation_steps) * num_train_epochs\n",
    "    step = 0\n",
    "\n",
    "    for epoch in range(num_train_epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, batch in enumerate(train_dataloader):\n",
    "            try:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                if loss is None or not loss.requires_grad:\n",
    "                    print(\"Error: Loss does not require grad or is None\")\n",
    "                    print(f\"Batch index: {batch_idx}\")\n",
    "                    continue\n",
    "\n",
    "                loss = loss / gradient_accumulation_steps\n",
    "                total_loss += loss.item() * gradient_accumulation_steps\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    step += 1\n",
    "                    print(f\"Epoch {epoch+1}, Step {step}/{total_steps}, Loss: {total_loss / (batch_idx + 1):.4f}\")\n",
    "\n",
    "            except torch.cuda.OutOfMemoryError as e:\n",
    "                print(f\"CUDA Out of Memory Error: {e}\")\n",
    "                print(\"Consider reducing batch size or enabling gradient accumulation.\")\n",
    "                torch.cuda.empty_cache()\n",
    "                continue\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Runtime Error during training: {e}\")\n",
    "                continue\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for val_batch in val_dataloader:\n",
    "                input_ids = val_batch[\"input_ids\"].to(device)\n",
    "                attention_mask = val_batch[\"attention_mask\"].to(device)\n",
    "                labels = val_batch[\"labels\"].to(device)\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                val_loss += outputs.loss.item()\n",
    "        val_loss /= len(val_dataloader)\n",
    "        print(f\"Epoch {epoch+1}, Validation Loss: {val_loss:.4f}\")\n",
    "        model.train()\n",
    "\n",
    "        local_save_path = f\"{output_dir}/{task_name}\"\n",
    "        os.makedirs(local_save_path, exist_ok=True)\n",
    "        model.save_pretrained(local_save_path)\n",
    "        tokenizer.save_pretrained(local_save_path)\n",
    "        print(f\"Model and tokenizer saved locally to {local_save_path}\")\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T15:40:26.805283Z",
     "iopub.status.busy": "2025-05-10T15:40:26.804779Z",
     "iopub.status.idle": "2025-05-10T15:40:26.808738Z",
     "shell.execute_reply": "2025-05-10T15:40:26.807964Z",
     "shell.execute_reply.started": "2025-05-10T15:40:26.805260Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "repo_name_qa_answer = \"DohaHemdann/mistral_qa_answer2epochs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T15:40:28.442081Z",
     "iopub.status.busy": "2025-05-10T15:40:28.441797Z",
     "iopub.status.idle": "2025-05-10T15:40:28.445653Z",
     "shell.execute_reply": "2025-05-10T15:40:28.444829Z",
     "shell.execute_reply.started": "2025-05-10T15:40:28.442060Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "access_token = \"HF_TOKEN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-05-10T15:40:30.332712Z",
     "iopub.status.busy": "2025-05-10T15:40:30.332446Z",
     "iopub.status.idle": "2025-05-10T19:28:56.905582Z",
     "shell.execute_reply": "2025-05-10T19:28:56.904973Z",
     "shell.execute_reply.started": "2025-05-10T15:40:30.332693Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training QA Answering Model...\n",
      "Epoch 1, Step 1/291, Loss: 2.3838\n",
      "Epoch 1, Step 2/291, Loss: 2.4389\n",
      "Epoch 1, Step 3/291, Loss: 2.4345\n",
      "Epoch 1, Step 4/291, Loss: 2.4421\n",
      "Epoch 1, Step 5/291, Loss: 2.4420\n",
      "Epoch 1, Step 6/291, Loss: 2.4432\n",
      "Epoch 1, Step 7/291, Loss: 2.4462\n",
      "Epoch 1, Step 8/291, Loss: 2.4452\n",
      "Epoch 1, Step 9/291, Loss: 2.4475\n",
      "Epoch 1, Step 10/291, Loss: 2.4453\n",
      "Epoch 1, Step 11/291, Loss: 2.4530\n",
      "Epoch 1, Step 12/291, Loss: 2.4562\n",
      "Epoch 1, Step 13/291, Loss: 2.4629\n",
      "Epoch 1, Step 14/291, Loss: 2.4590\n",
      "Epoch 1, Step 15/291, Loss: 2.4501\n",
      "Epoch 1, Step 16/291, Loss: 2.4492\n",
      "Epoch 1, Step 17/291, Loss: 2.4442\n",
      "Epoch 1, Step 18/291, Loss: 2.4402\n",
      "Epoch 1, Step 19/291, Loss: 2.4382\n",
      "Epoch 1, Step 20/291, Loss: 2.4380\n",
      "Epoch 1, Step 21/291, Loss: 2.4322\n",
      "Epoch 1, Step 22/291, Loss: 2.4244\n",
      "Epoch 1, Step 23/291, Loss: 2.4230\n",
      "Epoch 1, Step 24/291, Loss: 2.4220\n",
      "Epoch 1, Step 25/291, Loss: 2.4224\n",
      "Epoch 1, Step 26/291, Loss: 2.4213\n",
      "Epoch 1, Step 27/291, Loss: 2.4241\n",
      "Epoch 1, Step 28/291, Loss: 2.4229\n",
      "Epoch 1, Step 29/291, Loss: 2.4197\n",
      "Epoch 1, Step 30/291, Loss: 2.4189\n",
      "Epoch 1, Step 31/291, Loss: 2.4189\n",
      "Epoch 1, Step 32/291, Loss: 2.4177\n",
      "Epoch 1, Step 33/291, Loss: 2.4171\n",
      "Epoch 1, Step 34/291, Loss: 2.4172\n",
      "Epoch 1, Step 35/291, Loss: 2.4166\n",
      "Epoch 1, Step 36/291, Loss: 2.4149\n",
      "Epoch 1, Step 37/291, Loss: 2.4151\n",
      "Epoch 1, Step 38/291, Loss: 2.4157\n",
      "Epoch 1, Step 39/291, Loss: 2.4150\n",
      "Epoch 1, Step 40/291, Loss: 2.4142\n",
      "Epoch 1, Step 41/291, Loss: 2.4121\n",
      "Epoch 1, Step 42/291, Loss: 2.4106\n",
      "Epoch 1, Step 43/291, Loss: 2.4107\n",
      "Epoch 1, Step 44/291, Loss: 2.4077\n",
      "Epoch 1, Step 45/291, Loss: 2.4080\n",
      "Epoch 1, Step 46/291, Loss: 2.4070\n",
      "Epoch 1, Step 47/291, Loss: 2.4056\n",
      "Epoch 1, Step 48/291, Loss: 2.4038\n",
      "Epoch 1, Step 49/291, Loss: 2.4029\n",
      "Epoch 1, Step 50/291, Loss: 2.4025\n",
      "Epoch 1, Step 51/291, Loss: 2.4005\n",
      "Epoch 1, Step 52/291, Loss: 2.4005\n",
      "Epoch 1, Step 53/291, Loss: 2.3992\n",
      "Epoch 1, Step 54/291, Loss: 2.3984\n",
      "Epoch 1, Step 55/291, Loss: 2.3965\n",
      "Epoch 1, Step 56/291, Loss: 2.3943\n",
      "Epoch 1, Step 57/291, Loss: 2.3931\n",
      "Epoch 1, Step 58/291, Loss: 2.3923\n",
      "Epoch 1, Step 59/291, Loss: 2.3920\n",
      "Epoch 1, Step 60/291, Loss: 2.3912\n",
      "Epoch 1, Step 61/291, Loss: 2.3907\n",
      "Epoch 1, Step 62/291, Loss: 2.3906\n",
      "Epoch 1, Step 63/291, Loss: 2.3908\n",
      "Epoch 1, Step 64/291, Loss: 2.3905\n",
      "Epoch 1, Step 65/291, Loss: 2.3896\n",
      "Epoch 1, Step 66/291, Loss: 2.3893\n",
      "Epoch 1, Step 67/291, Loss: 2.3898\n",
      "Epoch 1, Step 68/291, Loss: 2.3887\n",
      "Epoch 1, Step 69/291, Loss: 2.3882\n",
      "Epoch 1, Step 70/291, Loss: 2.3883\n",
      "Epoch 1, Step 71/291, Loss: 2.3867\n",
      "Epoch 1, Step 72/291, Loss: 2.3867\n",
      "Epoch 1, Step 73/291, Loss: 2.3866\n",
      "Epoch 1, Step 74/291, Loss: 2.3851\n",
      "Epoch 1, Step 75/291, Loss: 2.3836\n",
      "Epoch 1, Step 76/291, Loss: 2.3834\n",
      "Epoch 1, Step 77/291, Loss: 2.3835\n",
      "Epoch 1, Step 78/291, Loss: 2.3817\n",
      "Epoch 1, Step 79/291, Loss: 2.3819\n",
      "Epoch 1, Step 80/291, Loss: 2.3816\n",
      "Epoch 1, Step 81/291, Loss: 2.3790\n",
      "Epoch 1, Step 82/291, Loss: 2.3791\n",
      "Epoch 1, Step 83/291, Loss: 2.3779\n",
      "Epoch 1, Step 84/291, Loss: 2.3786\n",
      "Epoch 1, Step 85/291, Loss: 2.3797\n",
      "Epoch 1, Step 86/291, Loss: 2.3798\n",
      "Epoch 1, Step 87/291, Loss: 2.3793\n",
      "Epoch 1, Step 88/291, Loss: 2.3794\n",
      "Epoch 1, Step 89/291, Loss: 2.3793\n",
      "Epoch 1, Step 90/291, Loss: 2.3793\n",
      "Epoch 1, Step 91/291, Loss: 2.3803\n",
      "Epoch 1, Step 92/291, Loss: 2.3798\n",
      "Epoch 1, Step 93/291, Loss: 2.3798\n",
      "Epoch 1, Step 94/291, Loss: 2.3815\n",
      "Epoch 1, Step 95/291, Loss: 2.3818\n",
      "Epoch 1, Step 96/291, Loss: 2.3825\n",
      "Epoch 1, Step 97/291, Loss: 2.3826\n",
      "Epoch 1, Validation Loss: 2.4423\n",
      "Model and tokenizer saved locally to ./mistral_finetuned/mistral_qa_answer\n",
      "Epoch 2, Step 98/291, Loss: 2.4467\n",
      "Epoch 2, Step 99/291, Loss: 2.4313\n",
      "Epoch 2, Step 100/291, Loss: 2.4479\n",
      "Epoch 2, Step 101/291, Loss: 2.4557\n",
      "Epoch 2, Step 102/291, Loss: 2.4259\n",
      "Epoch 2, Step 103/291, Loss: 2.4157\n",
      "Epoch 2, Step 104/291, Loss: 2.4221\n",
      "Epoch 2, Step 105/291, Loss: 2.4275\n",
      "Epoch 2, Step 106/291, Loss: 2.4305\n",
      "Epoch 2, Step 107/291, Loss: 2.4406\n",
      "Epoch 2, Step 108/291, Loss: 2.4399\n",
      "Epoch 2, Step 109/291, Loss: 2.4391\n",
      "Epoch 2, Step 110/291, Loss: 2.4359\n",
      "Epoch 2, Step 111/291, Loss: 2.4410\n",
      "Epoch 2, Step 112/291, Loss: 2.4454\n",
      "Epoch 2, Step 113/291, Loss: 2.4454\n",
      "Epoch 2, Step 114/291, Loss: 2.4465\n",
      "Epoch 2, Step 115/291, Loss: 2.4445\n",
      "Epoch 2, Step 116/291, Loss: 2.4467\n",
      "Epoch 2, Step 117/291, Loss: 2.4499\n",
      "Epoch 2, Step 118/291, Loss: 2.4455\n",
      "Epoch 2, Step 119/291, Loss: 2.4492\n",
      "Epoch 2, Step 120/291, Loss: 2.4466\n",
      "Epoch 2, Step 121/291, Loss: 2.4451\n",
      "Epoch 2, Step 122/291, Loss: 2.4451\n",
      "Epoch 2, Step 123/291, Loss: 2.4472\n",
      "Epoch 2, Step 124/291, Loss: 2.4516\n",
      "Epoch 2, Step 125/291, Loss: 2.4555\n",
      "Epoch 2, Step 126/291, Loss: 2.4580\n",
      "Epoch 2, Step 127/291, Loss: 2.4629\n",
      "Epoch 2, Step 128/291, Loss: 2.4647\n",
      "Epoch 2, Step 129/291, Loss: 2.4657\n",
      "Epoch 2, Step 130/291, Loss: 2.4711\n",
      "Epoch 2, Step 131/291, Loss: 2.4720\n",
      "Epoch 2, Step 132/291, Loss: 2.4754\n",
      "Epoch 2, Step 133/291, Loss: 2.4753\n",
      "Epoch 2, Step 134/291, Loss: 2.4776\n",
      "Epoch 2, Step 135/291, Loss: 2.4799\n",
      "Epoch 2, Step 136/291, Loss: 2.4814\n",
      "Epoch 2, Step 137/291, Loss: 2.4833\n",
      "Epoch 2, Step 138/291, Loss: 2.4855\n",
      "Epoch 2, Step 139/291, Loss: 2.4846\n",
      "Epoch 2, Step 140/291, Loss: 2.4852\n",
      "Epoch 2, Step 141/291, Loss: 2.4871\n",
      "Epoch 2, Step 142/291, Loss: 2.4890\n",
      "Epoch 2, Step 143/291, Loss: 2.4890\n",
      "Epoch 2, Step 144/291, Loss: 2.4897\n",
      "Epoch 2, Step 145/291, Loss: 2.4909\n",
      "Epoch 2, Step 146/291, Loss: 2.4907\n",
      "Epoch 2, Step 147/291, Loss: 2.4933\n",
      "Epoch 2, Step 148/291, Loss: 2.4940\n",
      "Epoch 2, Step 149/291, Loss: 2.4948\n",
      "Epoch 2, Step 150/291, Loss: 2.4950\n",
      "Epoch 2, Step 151/291, Loss: 2.4967\n",
      "Epoch 2, Step 152/291, Loss: 2.4982\n",
      "Epoch 2, Step 153/291, Loss: 2.4995\n",
      "Epoch 2, Step 154/291, Loss: 2.5019\n",
      "Epoch 2, Step 155/291, Loss: 2.5036\n",
      "Epoch 2, Step 156/291, Loss: 2.5021\n",
      "Epoch 2, Step 157/291, Loss: 2.5035\n",
      "Epoch 2, Step 158/291, Loss: 2.5053\n",
      "Epoch 2, Step 159/291, Loss: 2.5079\n",
      "Epoch 2, Step 160/291, Loss: 2.5086\n",
      "Epoch 2, Step 161/291, Loss: 2.5110\n",
      "Epoch 2, Step 162/291, Loss: 2.5118\n",
      "Epoch 2, Step 163/291, Loss: 2.5122\n",
      "Epoch 2, Step 164/291, Loss: 2.5140\n",
      "Epoch 2, Step 165/291, Loss: 2.5155\n",
      "Epoch 2, Step 166/291, Loss: 2.5167\n",
      "Epoch 2, Step 167/291, Loss: 2.5182\n",
      "Epoch 2, Step 168/291, Loss: 2.5196\n",
      "Epoch 2, Step 169/291, Loss: 2.5223\n",
      "Epoch 2, Step 170/291, Loss: 2.5233\n",
      "Epoch 2, Step 171/291, Loss: 2.5230\n",
      "Epoch 2, Step 172/291, Loss: 2.5231\n",
      "Epoch 2, Step 173/291, Loss: 2.5240\n",
      "Epoch 2, Step 174/291, Loss: 2.5228\n",
      "Epoch 2, Step 175/291, Loss: 2.5223\n",
      "Epoch 2, Step 176/291, Loss: 2.5236\n",
      "Epoch 2, Step 177/291, Loss: 2.5228\n",
      "Epoch 2, Step 178/291, Loss: 2.5230\n",
      "Epoch 2, Step 179/291, Loss: 2.5246\n",
      "Epoch 2, Step 180/291, Loss: 2.5247\n",
      "Epoch 2, Step 181/291, Loss: 2.5249\n",
      "Epoch 2, Step 182/291, Loss: 2.5265\n",
      "Epoch 2, Step 183/291, Loss: 2.5271\n",
      "Epoch 2, Step 184/291, Loss: 2.5260\n",
      "Epoch 2, Step 185/291, Loss: 2.5257\n",
      "Epoch 2, Step 186/291, Loss: 2.5259\n",
      "Epoch 2, Step 187/291, Loss: 2.5270\n",
      "Epoch 2, Step 188/291, Loss: 2.5282\n",
      "Epoch 2, Step 189/291, Loss: 2.5293\n",
      "Epoch 2, Step 190/291, Loss: 2.5312\n",
      "Epoch 2, Step 191/291, Loss: 2.5310\n",
      "Epoch 2, Step 192/291, Loss: 2.5318\n",
      "Epoch 2, Step 193/291, Loss: 2.5318\n",
      "Epoch 2, Step 194/291, Loss: 2.5320\n",
      "Epoch 2, Validation Loss: 2.5733\n",
      "Model and tokenizer saved locally to ./mistral_finetuned/mistral_qa_answer\n",
      "Epoch 3, Step 195/291, Loss: 2.4556\n",
      "Epoch 3, Step 196/291, Loss: 2.5323\n",
      "Epoch 3, Step 197/291, Loss: 2.5506\n",
      "Epoch 3, Step 198/291, Loss: 2.5707\n",
      "Epoch 3, Step 199/291, Loss: 2.5759\n",
      "Epoch 3, Step 200/291, Loss: 2.5865\n",
      "Epoch 3, Step 201/291, Loss: 2.5877\n",
      "Epoch 3, Step 202/291, Loss: 2.5827\n",
      "Epoch 3, Step 203/291, Loss: 2.5788\n",
      "Epoch 3, Step 204/291, Loss: 2.5782\n",
      "Epoch 3, Step 205/291, Loss: 2.5683\n",
      "Epoch 3, Step 206/291, Loss: 2.5586\n",
      "Epoch 3, Step 207/291, Loss: 2.5554\n",
      "Epoch 3, Step 208/291, Loss: 2.5545\n",
      "Epoch 3, Step 209/291, Loss: 2.5581\n",
      "Epoch 3, Step 210/291, Loss: 2.5567\n",
      "Epoch 3, Step 211/291, Loss: 2.5559\n",
      "Epoch 3, Step 212/291, Loss: 2.5568\n",
      "Epoch 3, Step 213/291, Loss: 2.5542\n",
      "Epoch 3, Step 214/291, Loss: 2.5569\n",
      "Epoch 3, Step 215/291, Loss: 2.5592\n",
      "Epoch 3, Step 216/291, Loss: 2.5619\n",
      "Epoch 3, Step 217/291, Loss: 2.5647\n",
      "Epoch 3, Step 218/291, Loss: 2.5680\n",
      "Epoch 3, Step 219/291, Loss: 2.5672\n",
      "Epoch 3, Step 220/291, Loss: 2.5664\n",
      "Epoch 3, Step 221/291, Loss: 2.5668\n",
      "Epoch 3, Step 222/291, Loss: 2.5652\n",
      "Epoch 3, Step 223/291, Loss: 2.5636\n",
      "Epoch 3, Step 224/291, Loss: 2.5645\n",
      "Epoch 3, Step 225/291, Loss: 2.5627\n",
      "Epoch 3, Step 226/291, Loss: 2.5666\n",
      "Epoch 3, Step 227/291, Loss: 2.5672\n",
      "Epoch 3, Step 228/291, Loss: 2.5675\n",
      "Epoch 3, Step 229/291, Loss: 2.5685\n",
      "Epoch 3, Step 230/291, Loss: 2.5683\n",
      "Epoch 3, Step 231/291, Loss: 2.5676\n",
      "Epoch 3, Step 232/291, Loss: 2.5697\n",
      "Epoch 3, Step 233/291, Loss: 2.5690\n",
      "Epoch 3, Step 234/291, Loss: 2.5687\n",
      "Epoch 3, Step 235/291, Loss: 2.5675\n",
      "Epoch 3, Step 236/291, Loss: 2.5675\n",
      "Epoch 3, Step 237/291, Loss: 2.5655\n",
      "Epoch 3, Step 238/291, Loss: 2.5647\n",
      "Epoch 3, Step 239/291, Loss: 2.5628\n",
      "Epoch 3, Step 240/291, Loss: 2.5622\n",
      "Epoch 3, Step 241/291, Loss: 2.5619\n",
      "Epoch 3, Step 242/291, Loss: 2.5597\n",
      "Epoch 3, Step 243/291, Loss: 2.5602\n",
      "Epoch 3, Step 244/291, Loss: 2.5602\n",
      "Epoch 3, Step 245/291, Loss: 2.5591\n",
      "Epoch 3, Step 246/291, Loss: 2.5585\n",
      "Epoch 3, Step 247/291, Loss: 2.5577\n",
      "Epoch 3, Step 248/291, Loss: 2.5568\n",
      "Epoch 3, Step 249/291, Loss: 2.5575\n",
      "Epoch 3, Step 250/291, Loss: 2.5579\n",
      "Epoch 3, Step 251/291, Loss: 2.5571\n",
      "Epoch 3, Step 252/291, Loss: 2.5565\n",
      "Epoch 3, Step 253/291, Loss: 2.5542\n",
      "Epoch 3, Step 254/291, Loss: 2.5533\n",
      "Epoch 3, Step 255/291, Loss: 2.5529\n",
      "Epoch 3, Step 256/291, Loss: 2.5515\n",
      "Epoch 3, Step 257/291, Loss: 2.5509\n",
      "Epoch 3, Step 258/291, Loss: 2.5486\n",
      "Epoch 3, Step 259/291, Loss: 2.5494\n",
      "Epoch 3, Step 260/291, Loss: 2.5488\n",
      "Epoch 3, Step 261/291, Loss: 2.5491\n",
      "Epoch 3, Step 262/291, Loss: 2.5491\n",
      "Epoch 3, Step 263/291, Loss: 2.5495\n",
      "Epoch 3, Step 264/291, Loss: 2.5477\n",
      "Epoch 3, Step 265/291, Loss: 2.5470\n",
      "Epoch 3, Step 266/291, Loss: 2.5468\n",
      "Epoch 3, Step 267/291, Loss: 2.5467\n",
      "Epoch 3, Step 268/291, Loss: 2.5476\n",
      "Epoch 3, Step 269/291, Loss: 2.5476\n",
      "Epoch 3, Step 270/291, Loss: 2.5478\n",
      "Epoch 3, Step 271/291, Loss: 2.5471\n",
      "Epoch 3, Step 272/291, Loss: 2.5461\n",
      "Epoch 3, Step 273/291, Loss: 2.5446\n",
      "Epoch 3, Step 274/291, Loss: 2.5455\n",
      "Epoch 3, Step 275/291, Loss: 2.5451\n",
      "Epoch 3, Step 276/291, Loss: 2.5447\n",
      "Epoch 3, Step 277/291, Loss: 2.5445\n",
      "Epoch 3, Step 278/291, Loss: 2.5434\n",
      "Epoch 3, Step 279/291, Loss: 2.5430\n",
      "Epoch 3, Step 280/291, Loss: 2.5427\n",
      "Epoch 3, Step 281/291, Loss: 2.5420\n",
      "Epoch 3, Step 282/291, Loss: 2.5424\n",
      "Epoch 3, Step 283/291, Loss: 2.5417\n",
      "Epoch 3, Step 284/291, Loss: 2.5415\n",
      "Epoch 3, Step 285/291, Loss: 2.5418\n",
      "Epoch 3, Step 286/291, Loss: 2.5411\n",
      "Epoch 3, Step 287/291, Loss: 2.5410\n",
      "Epoch 3, Step 288/291, Loss: 2.5409\n",
      "Epoch 3, Step 289/291, Loss: 2.5409\n",
      "Epoch 3, Step 290/291, Loss: 2.5403\n",
      "Epoch 3, Step 291/291, Loss: 2.5401\n",
      "Epoch 3, Validation Loss: 2.5202\n",
      "Model and tokenizer saved locally to ./mistral_finetuned/mistral_qa_answer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): MistralForCausalLM(\n",
       "      (model): MistralModel(\n",
       "        (embed_tokens): Embedding(32000, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x MistralDecoderLayer(\n",
       "            (self_attn): MistralAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): MistralMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): MistralRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Training QA Answering Model...\")\n",
    "train_model(model, train_dataloader_qa_answer, val_dataloader_qa_answer, output_dir, \"mistral_qa_answer\", repo_name_qa_answer, access_token, tokenizer, optimizer, num_train_epochs,gradient_accumulation_steps, device )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T19:28:56.906845Z",
     "iopub.status.busy": "2025-05-10T19:28:56.906656Z",
     "iopub.status.idle": "2025-05-10T19:28:56.910304Z",
     "shell.execute_reply": "2025-05-10T19:28:56.909655Z",
     "shell.execute_reply.started": "2025-05-10T19:28:56.906831Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "repo_name = \"Dohahemdann/mistral_qa_answer2epochs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T19:28:56.911610Z",
     "iopub.status.busy": "2025-05-10T19:28:56.911044Z",
     "iopub.status.idle": "2025-05-10T19:28:56.926156Z",
     "shell.execute_reply": "2025-05-10T19:28:56.925543Z",
     "shell.execute_reply.started": "2025-05-10T19:28:56.911593Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "access_token = \"HF_TOKEN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T19:28:56.927769Z",
     "iopub.status.busy": "2025-05-10T19:28:56.927536Z",
     "iopub.status.idle": "2025-05-10T19:29:06.629542Z",
     "shell.execute_reply": "2025-05-10T19:29:06.628865Z",
     "shell.execute_reply.started": "2025-05-10T19:28:56.927754Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b2ac61e3f5f417899bd825c14ddff4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/13.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7d8466b5a794bc7a9771e73c8a0a5bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b1b784f44e14ac2861d6c1eaaa02dfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Dohahemdann/mistral_qa_answer2epochs/commit/4d626b99959ddb131f8b72f4b614283348d812b6', commit_message='Upload tokenizer', commit_description='', oid='4d626b99959ddb131f8b72f4b614283348d812b6', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Dohahemdann/mistral_qa_answer2epochs', endpoint='https://huggingface.co', repo_type='model', repo_id='Dohahemdann/mistral_qa_answer2epochs'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(repo_name, token=access_token)\n",
    "tokenizer.push_to_hub(repo_name, token=access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 2655917,
     "sourceId": 4549051,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
